---
title: "AMPH 2025: Forecasting with Time Series Models"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
editor_options:
  chunk_output_type: console
---

# Setup

## Set parameters

```{r set-params}
state_name <- "Maryland"
geo_ids <- "md"
forecast_date <- as.Date("2025-10-12")
forecast_disease <- "influenza"
target <- "wk inc flu hosp"
forecast_horizon_wks <- 0:3
```

## Install packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AMPHForecastSuite)
# Run `remotes::install_github("ACCIDDA/AMPH_Forecast_Suite")` if the package is not installed.

library(tidyverse)
library(forecast)
library(jsonlite)
library(ggplot2)
library(epidatr)

# Load packages
# library(AMPHForecastSuite)
library(epipredict)
library(epiprocess)
```

## Load target (observed) data

We already pulled and saved the data we need in `Collect Empirical Data`. See `vignettes/collect_empirical_data.Rmd` for details.

```{r read-target}

# Load data saved from a specific forecast date: 
target_data_path <- file.path("target-data", paste0("target-hospital-admissions-", forecast_date, ".csv"))
target_data <- readr::read_csv(file = target_data_path)


```

## Define Reference date
```{r define-ref-date}
# reference date is the saturday following the forecast date
reference_date <- get_reference_date(forecast_date)

message("Reference date: ", reference_date)
message("Forecast date: ", forecast_date)
```



# Models


## SARIMA (`forecast` package)

Here we use the `auto.arima` function from the `forecast` package to fit a seasonal ARIMA model to the data.

### Set the name
```{r}
model_name <- "AMPH-sarima"
```

```{r sarima-model}

fit_sarima <- forecast::auto.arima(target_data$value, seasonal = T, lambda = "auto")

fc_sarima <- fit_sarima %>%
  forecast(h = length(forecast_horizon_wks),
           level = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98))

data_fc_sarima <- fc_sarima %>%
  as.data.frame() %>%
  mutate(horizon = forecast_horizon_wks,
         origin_date = as.Date(target_data$time_value[length(target_data$time_value)]) + 7,
         location_id = state_name,
         target = target,
         model = model_name,
         output_type = "quantile") %>%
  rename(`Med 50` = `Point Forecast`) %>%
  pivot_longer(`Med 50`:`Hi 98`) %>%
  separate(name, into = c("interval", "confidence")) %>%
  mutate(output_type_id = case_when(interval == "Lo" ~ (50 - as.numeric(confidence)/2)/100,
                                    interval == "Hi" ~ (50 + as.numeric(confidence)/2)/100,
                                    interval == "Med" ~ 50/100),
         value = as.numeric(value),
         output_type = "quantile") %>%
  arrange(horizon, output_type_id) %>%
  select(horizon, origin_date, location_id, target, model, output_type_id, value, output_type)

```

### Check that the model output is in the correct format.
We use the `hubValidations` package to check against the `tasks.json` file we create in `Getting Started`.

```{r}
## --> FIX LATER <---
# hub_path <- system.file("testhubs/simple", package = "hubValidations")
# file_path <- "team1-goodmodel/2022-10-08-team1-goodmodel.csv"
# hubValidations::validate_submission(hub_path, file_path)

```

### Save in hubVerse Format

```{r save-sarima-output}
## Save data file
#' -- this will have validation build in for fluid workflow eventually.
save_model_output(model_name = model_name,
                  fc_output = data_fc_sarima)

```



## Neural network model (`forecast` package)

Here we use the `nnetar` function from the `forecast` package to fit a neural network model to the data.

### Set the name
```{r}
model_name <- "AMPH-neuralnetwork"
```


```{r neuralnet-model}

fit_nnet <- forecast::nnetar(target_data$value, lambda = "auto")

fc_nnet <- forecast(fit_nnet,
                    PI = TRUE,
                    h = length(forecast_horizon_wks),
                    level = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.98))

data_fc_nnet <- fc_nnet %>%
  as.data.frame() %>%
  mutate(horizon = forecast_horizon_wks,
         origin_date = as.Date(target_data$time_value[length(target_data$time_value)]) + 7,
         location_id = state_name,
         target = target,
         model = model_name,
         output_type = "quantile") %>%
  rename(`Med 50` = `Point Forecast`) %>%
  pivot_longer(`Med 50`:`Hi 98`) %>%
  separate(name, into = c("interval", "confidence")) %>%
  mutate(output_type_id = case_when(interval == "Lo" ~ (50 - as.numeric(confidence)/2)/100,
                                    interval == "Hi" ~ (50 + as.numeric(confidence)/2)/100,
                                    interval == "Med" ~ 50/100),
         value = as.numeric(value),
         output_type = "quantile") %>%
  arrange(horizon, output_type_id) %>%
  select(horizon, origin_date, location_id, target, model, output_type_id, value, output_type)

```

### Save in hubVerse Format

```{r save-neuralnet-output}
## Save data file
#' -- this will have validation build in for fluid workflow eventually.
save_model_output(model_name = model_name,
                  fc_output = data_fc_nnet)

```



## Autoregressive Forecaster (`epipredict` package)    

Here we use the `epipredict` package to fit an autoregressive model to the data. We specify lags of 0, 7, and 14 days (i.e., current week, previous week, and two weeks ago) and forecast horizons of 7, 14, 21, and 28 days ahead (i.e., 1 to 4 weeks ahead).

### Set the name
```{r}
model_name <- "AMPH-epipredict-arx"
```

```{r}

# Set up data for epipredict
target_data_arx <- target_data %>%
  tsibble::as_tsibble(index = time_value, key = c(geo_value)) %>%
  arrange(geo_value, time_value) %>%
  epiprocess::as_epi_df()

# Run model
arx_forecast <- lapply(
  seq(7, 28, 7),
  function(days_ahead) {
    epipredict::arx_forecaster(
      epi_data = target_data_arx, 
      outcome = "value",
      trainer = linear_reg(),
      predictors = "value",
      args_list = arx_args_list(
        lags = list(c(0, 7, 14)),
        ahead = days_ahead,
        quantile_levels = c(0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35,
                            0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7,
                            0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99),
        nonneg = TRUE
      )
    )
  }
)

# pull out the workflow and the predictions to be able to effectively use autoplot
arx_forecaster_workflow <- arx_forecast[[1]]$epi_workflow
arx_forecaster_results <- arx_forecast %>%
  purrr::map(~ `$`(., "predictions")) %>%
  list_rbind() %>%
  mutate(horizon = forecast_horizon_wks,
         location_id = state_name,
         model = model_name,
         target = target)
autoplot(
  object = arx_forecaster_workflow,
  predictions = arx_forecaster_results,
  observed_response = target_data_arx %>%
    filter(geo_value %in% geo_ids, time_value > "2025-06-01")) +
  geom_vline(aes(xintercept = forecast_date))

```

### Save in hubVerse Format

```{r save-neuralnet-output}

# Transform to hubVerse format
data_fc_arx_epipred <- trans_epipredarx_hv(fc_output = arx_forecast,
                                           model_name = model_name,
                                           target = target,
                                           reference_date = reference_date,
                                           horizon_time_steps = forecast_horizon_wks)

## Save data file
#' -- this will have validation build in for fluid workflow eventually.
save_model_output(model_name = model_name,
                  fc_output = data_fc_arx_epipred)

```



## Climatological Forecaster (`epipredict` package)    

Here we use the `epipredict` package to fit a climatological model to the data. We specify a forecast horizon of 7, 14, 21, and 28 days ahead (i.e., 1 to 4 weeks ahead).

### Set the name
```{r}
model_name <- "AMPH-epipredict-climate"
```


```{r}
# Set up data for epipredict
target_data_clim <- target_data %>%
  tsibble::as_tsibble(index = time_value, key = c(geo_value)) %>%
  arrange(geo_value, time_value) %>%
  epiprocess::as_epi_df()

# Run the model
climate_forecast <- epipredict::climatological_forecaster(
  target_data_clim,
  outcome = "value",
  args_list = climate_args_list(
    forecast_horizon = forecast_horizon_wks,
    time_type = "week",
    quantile_levels = c(0.01, 0.025, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35,
                        0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7,
                        0.75, 0.8, 0.85, 0.9, 0.95, 0.975, 0.99),
    center_method = "mean",
    quantile_by_key = "geo_value",
    forecast_date = forecast_date,
    nonneg = TRUE
  )
)
climate_forecast_workflow <- climate_forecast$epi_workflow
climate_forecast_results <- climate_forecast$predictions %>%
  mutate(horizon = forecast_horizon_wks,
         location_id = state_name,
         model = mod_name,
         target = target)
autoplot(
  object = climate_forecast_workflow,
  predictions = climate_forecast_results,
  observed_response = target_data_clim %>%
    filter(geo_value %in% geo_ids, time_value > "2025-06-01")) +
  geom_vline(aes(xintercept = forecast_date))
```

### Save in hubVerse Format

```{r save-neuralnet-output}

# Transform to hubVerse format
hub_forecast <- trans_epipredclim_hv(fc_output = climate_forecast,
                                     model_name = model_name,
                                     target = target,
                                     reference_date = reference_date,
                                     horizon_time_steps = forecast_horizon_wks)

## Save data file
#' -- this will have validation build in for fluid workflow eventually.
save_model_output(model_name = mod_name,
                  fc_output = data_fc_arx_epipred)
```
