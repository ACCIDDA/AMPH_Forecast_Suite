---
title: "AMPH 2025: Simple Ensemble, Visualization, and Scoring Using Hub Model Output"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE)

```

# 1) Setup

Install & load required R packages

```{r packages}
# Install CRAN deps first (safe if already installed)
cran_pkgs <- c(
  "remotes", "jsonlite", "readr", "dplyr", "purrr", "tidyr",
  "ggplot2", "viridisLite"
)
to_install <- setdiff(cran_pkgs, rownames(installed.packages()))
if (length(to_install)) install.packages(to_install, dependencies = TRUE)

# Install hubverse packages from GitHub if missing
gh_pkgs <- c("hubverse-org/hubEnsembles", "hubverse-org/hubUtils", "hubverse-org/hubVis",
             "epiforecasts/scoringutils")
need_gh <- c(!requireNamespace("hubEnsembles", quietly = TRUE),
             !requireNamespace("hubUtils", quietly = TRUE),
             !requireNamespace("hubVis", quietly = TRUE),
             !requireNamespace("scoringutils", quietly = TRUE))
if (any(need_gh)) {
  remotes::install_github(gh_pkgs[need_gh], upgrade = "never", dependencies = TRUE)
}

library(hubEnsembles)
library(hubUtils)
library(hubVis)
library(scoringutils)
library(dplyr)
library(purrr)
library(ggplot2)
library(jsonlite)
library(readr)
library(tidyr)

```



# 2) Get FluSight data repo

The FluSight Github repository stores forecast data for the 2023-2024 FluSight collaborative exercise run by the US CDC. This project collects forecasts for weekly new hospitalizations due to confirmed influenza. More information can be found in the ReadMe of the repository: https://github.com/cdcepi/FluSight-forecast-hub


```{r clone-repo}

# Ensure Git is available
has_git <- tryCatch(
  system2("git", "--version", stdout = TRUE, stderr = TRUE),
  error = function(e) NA
)
if (any(is.na(has_git))) stop("Git does not appear to be installed or on PATH.")

repo_dir <- "FluSight-forecast-hub"

# Clone into working directory if not present
if (!dir.exists(repo_dir)) {
  message("Cloning repository...")
  status <- system2("git", c("clone", "https://github.com/cdcepi/FluSight-forecast-hub.git"),
                    stdout = TRUE, stderr = TRUE)
  cat(paste(status, collapse = "\n"), "\n")
} else {
  message("Repository exists. Pulling latest changes...")

  # Check it's actually a git repo
  is_repo <- tryCatch(
    system2("git", c("-C", repo_dir, "rev-parse", "--is-inside-work-tree"),
            stdout = TRUE, stderr = TRUE),
    error = function(e) "false"
  )

  if (identical(trimws(is_repo), "true")) {
    # Warn if there are local changes
    dirty <- trimws(paste(system2("git", c("-C", repo_dir, "status", "--porcelain"),
                                  stdout = TRUE), collapse = "\n"))
    if (nchar(dirty) > 0) {
      message("⚠️ Local changes detected in ", repo_dir, 
              ". Pulling with --ff-only (won't overwrite local work).")
    }

    # Fetch and pull (fast-forward only)
    fetch_out <- system2("git", c("-C", repo_dir, "fetch", "--prune"),
                         stdout = TRUE, stderr = TRUE)
    pull_out  <- system2("git", c("-C", repo_dir, "pull", "--ff-only", "--quiet"),
                         stdout = TRUE, stderr = TRUE)

    # Show any messages from Git
    if (length(fetch_out)) cat(paste(fetch_out, collapse = "\n"), "\n")
    if (length(pull_out))  cat(paste(pull_out,  collapse = "\n"), "\n")
    message("Pull complete.")
  } else {
    stop(sprintf("Path '%s' exists but is not a Git repository.", repo_dir))
  }
}

# Normalize path for downstream code
dir_path <- normalizePath(repo_dir)
message("Using dir_path: ", dir_path)

```


# 3) Choose forecast round (reference date)

```{r choose-round}
# Read submission (reference) dates from hub-config/tasks.json
tasks_path <- file.path(dir_path, "hub-config", "tasks.json")
tasks <- jsonlite::read_json(tasks_path)

# Extract reference dates from the first round / first model task
dates_archive <- unlist(tasks$rounds[[1]]$model_tasks[[1]]$task_ids$reference_date$optional)
dates_archive <- as.Date(dates_archive)
dates_archive <- dates_archive[dates_archive <= Sys.Date()]

# ---- Set reference date of interest ----
# Option 1: most recent submission date
# curr_origin_date <- max(dates_archive, na.rm = TRUE)

# Option 2: explicit date (must be one of the submission dates)
curr_origin_date <- as.Date("2025-01-18")

# Option 3: pick by index (e.g., 12th date)
# curr_origin_date <- dates_archive[12]

if (!curr_origin_date %in% dates_archive) {
  stop("Chosen curr_origin_date is not a valid submission date in the Hub.")
}

curr_origin_date
```


# 4) Load model output (submitted forecasts)

```{r read-forecasts}
output_path <- file.path(dir_path, "model-output")

# Retrieve parquet/csv model output files and keep those matching the reference date
file_paths <- list.files(output_path, pattern = "\\.(parquet|csv)$",
                         full.names = TRUE, recursive = TRUE)
file_paths <- file_paths[grepl(curr_origin_date, file_paths)]

if (!length(file_paths)) {
  stop("No model-output files found for curr_origin_date = ", curr_origin_date,
       ". Try a different date.")
}


# helper that picks the right reader
read_model_file <- function(path) {
  if (grepl("\\.parquet(\\.gz)?$", path, ignore.case = TRUE)) {
    # use arrow for parquet (handles both .parquet and .parquet.gz)
    arrow::read_parquet(path)
  } else if (grepl("\\.csv(\\.gz)?$", path, ignore.case = TRUE)) {
    readr::read_csv(path, show_col_types = FALSE)
  } else {
    stop("Unrecognized file type: ", path)
  }
}

# Read & bind; keep quantile forecasts; add model_id from folder name

projection_data_all <- file_paths %>%
  purrr::map_dfr(function(.x) {
    df <- read_model_file(.x)

    # standardize expected columns just in case
    if (!"output_type" %in% names(df))   stop("Missing 'output_type' in: ", .x)
    if (!"output_type_id" %in% names(df)) stop("Missing 'output_type_id' in: ", .x)

    df %>%
      dplyr::filter(.data$output_type == "quantile") %>%
      dplyr::mutate(
        output_type_id = suppressWarnings(as.numeric(.data$output_type_id)),
        model_id = basename(dirname(.x))
      )
  })


# Convert to hubverse model_out_tbl format
projection_data_tbl <- hubUtils::as_model_out_tbl(projection_data_all) %>%
  dplyr::filter(model_id %in% c(
    "FluSight-baseline",
    "MOBS-GLEAM_FLUH",
    "AMPH-SARIMA",
    "AMPH-neuralnetwork"
  ))

# Read and join location metadata (for names/abbreviations)
loc_data <- readr::read_csv(file.path(dir_path, "auxiliary-data", "locations.csv"),
                            show_col_types = FALSE)

projection_data_tbl <- projection_data_tbl %>%
  dplyr::left_join(
    loc_data %>%
      dplyr::select(location, location_id = location_name, loc_abbr = abbreviation),
    by = "location"
  )

dplyr::distinct(projection_data_tbl, model_id)

```



# 5) Load target (observed) data

```{r read-target}

# # If loading most recent data: 
# target_path <- file.path(dir_path, "target-data", "time-series.csv")
# target_data_all <- readr::read_csv(target_path, show_col_types = FALSE)


# If loading from a specific date: 
date_str <- format(as.Date(curr_origin_date), "%Y-%m-%d")
target_filename <- paste0(date_str, "_flu_target_hospital_admissions_data.csv")

target_path <- file.path(
  dir_path,
  "weekly-summaries",
  date_str,
  target_filename
)

target_data_all <- readr::read_csv(target_path, show_col_types = FALSE)


```



# 6) Pick location, start date, and uncertainty bands

```{r params}
# Location can be "US" or a full state name (must match location_name in target_data)
loc <- "Maryland"
start_date <- as.Date("2024-11-23")
# Middle 50% interval:
uncertainty <- c(0.25, 0.75)
uncertainty

```


# 7) Build a simple equal-weight ensemble

```{r ensemble}
# Filter submitted projections to the location of interest
projection_data <- projection_data_tbl %>%
  dplyr::filter(.data$location_id == loc)

# Keep only the chosen forecast round
round_dat <- projection_data %>%
  dplyr::filter(.data$reference_date == as.Date(curr_origin_date)) %>%
  dplyr::collect()

# Generate a simple (equal-weight) ensemble across contributing models
round_ens <- hubEnsembles::simple_ensemble(round_dat)

# Combine ensemble with individual models for plotting
plot_df <- dplyr::bind_rows(round_dat, round_ens)

unique(plot_df$model_id)
```



# 8) Prepare data for visualization

```{r viz-prep}
# Forecasts to tidy plotting format
proj_data <- hubUtils::as_model_out_tbl(plot_df) %>%
  dplyr::rename(target_date = target_end_date)

# Observed data for the same location and time window
target_data <- target_data_all %>%
  dplyr::filter(.data$location_name == loc, 
                .data$week_ending_date > start_date) %>%
  dplyr::rename(observation = value)

head(proj_data)
head(target_data)

```



# 9) Plot forecasts vs. truth

```{r plot, fig.width=9, fig.height=6}
ggplot() +
  # Uncertainty ribbon per model (e.g., 25%–75%)
  geom_ribbon(
    data = proj_data %>%
      dplyr::filter(output_type_id %in% uncertainty) %>%
      tidyr::pivot_wider(
        names_from  = output_type_id,
        values_from = value,
        names_prefix = "q"
      ),
    aes(
      x = target_date,
      ymin = .data[[paste0("q", uncertainty[1])]],
      ymax = .data[[paste0("q", uncertainty[2])]],
      fill = model_id,
      group = model_id
    ),
    alpha = 0.25
  ) +
  # Median forecast lines/points per model
  geom_line(
    data = proj_data %>% dplyr::filter(output_type_id == 0.5),
    aes(x = target_date, y = value, color = model_id),
    linewidth = 0.7
  ) +
  geom_point(
    data = proj_data %>% dplyr::filter(output_type_id == 0.5),
    aes(x = target_date, y = value, color = model_id),
    size = 0.9
  ) +
  # Observed truth
  geom_line(
    data = target_data,
    aes(x = week_ending_date, y = observation),
    linewidth = 0.9
  ) +
  geom_point(
    data = target_data,
    aes(x = week_ending_date, y = observation),
    size = 1.2
  ) +
  scale_x_date(date_breaks = "2 weeks", labels = scales::label_date("%m-%d-%Y")) +
  scale_color_viridis_d(option = "H", begin = 0.1, end = 0.9) +
  scale_fill_viridis_d(option = "H", begin = 0.1, end = 0.9) +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom") +
  guides(fill = "none") +  # avoid duplicate fill/color legends
  labs(
    x = "Date",
    y = "Weekly New Influenza Hospitalizations",
    color = "Model / Ensemble"
  )
```



# 10) Score forecasts (WIS, coverage)

```{r scoring-prep}

score_date <- format(curr_origin_date + 28, "%Y-%m-%d")
scoretarget_filename <- paste0(score_date, "_flu_target_hospital_admissions_data.csv")

score_path <- file.path(
  dir_path,
  "weekly-summaries",
  score_date,
  scoretarget_filename
)

scoring_target_data <- readr::read_csv(score_path, show_col_types = FALSE)

```

```{r scoring}
# Join forecasts with observations at (target_date, location)
# and conform to scoringutils "forecast" structure.

scoring_df <- dplyr::left_join(
  proj_data,
  scoring_target_data %>% dplyr::rename(target_date = week_ending_date,
                                        observation = value),
  by = c("target_date", "location"),
  relationship = "many-to-one"
) %>%
  dplyr::rename(
    model = model_id,
    predicted = value,
    observed = observation,
    quantile_level = output_type_id
  )

# Convert to a scoringutils forecast object
forecast <- scoringutils::as_forecast_quantile(
  scoring_df,
  observed       = "observed",
  predicted      = "predicted",
  quantile_level = "quantile_level",
  # be explicit so extra cols don't confuse the unit of a single forecast
  forecast_unit  = c("model", "location", "target_date")
)

# Score (WIS, coverage, etc.)
scores <- scoringutils::score(forecast)

scoringutils::summarise_scores(scores, by = "model")

```



