---
title: "AMPH 2025: Simple Ensemble, Visualization, and Scoring Using Hub Model Output"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
editor_options:
  chunk_output_type: console
---

# 0) Set parameters

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE)

state_name <- "Maryland"
used_locations <- "md"
forecast_date <- as.Date("2024-11-30")
```

# 1) Setup

Install & load required R packages

```{r packages}

library(hubEnsembles)
library(hubUtils)
library(hubVis)
library(scoringutils)
library(dplyr)
library(purrr)
library(ggplot2)
library(jsonlite)
library(readr)
library(tidyr)
library(scales)

```



# 2) Get FluSight data repo

The FluSight Github repository stores forecast data for the 2023-2024 FluSight collaborative exercise run by the US CDC. This project collects forecasts for weekly new hospitalizations due to confirmed influenza. More information can be found in the ReadMe of the repository: https://github.com/cdcepi/FluSight-forecast-hub


```{r clone-repo}

# Ensure Git is available
has_git <- tryCatch(
  system2("git", "--version", stdout = TRUE, stderr = TRUE),
  error = function(e) NA
)
if (any(is.na(has_git))) stop("Git does not appear to be installed or on PATH.")

repo_dir <- "FluSight-forecast-hub"

# Clone into working directory if not present
if (!dir.exists(repo_dir)) {
  message("Cloning repository...")
  status <- system2("git", c("clone", "https://github.com/cdcepi/FluSight-forecast-hub.git"),
                    stdout = TRUE, stderr = TRUE)
  cat(paste(status, collapse = "\n"), "\n")
} else {
  message("Repository exists. Pulling latest changes...")

  # Check it's actually a git repo
  is_repo <- tryCatch(
    system2("git", c("-C", repo_dir, "rev-parse", "--is-inside-work-tree"),
            stdout = TRUE, stderr = TRUE),
    error = function(e) "false"
  )

  if (identical(trimws(is_repo), "true")) {
    # Warn if there are local changes
    dirty <- trimws(paste(system2("git", c("-C", repo_dir, "status", "--porcelain"),
                                  stdout = TRUE), collapse = "\n"))
    if (nchar(dirty) > 0) {
      message("⚠️ Local changes detected in ", repo_dir, 
              ". Pulling with --ff-only (won't overwrite local work).")
    }

    # Fetch and pull (fast-forward only)
    fetch_out <- system2("git", c("-C", repo_dir, "fetch", "--prune"),
                         stdout = TRUE, stderr = TRUE)
    pull_out  <- system2("git", c("-C", repo_dir, "pull", "--ff-only", "--quiet"),
                         stdout = TRUE, stderr = TRUE)

    # Show any messages from Git
    if (length(fetch_out)) cat(paste(fetch_out, collapse = "\n"), "\n")
    if (length(pull_out))  cat(paste(pull_out,  collapse = "\n"), "\n")
    message("Pull complete.")
  } else {
    stop(sprintf("Path '%s' exists but is not a Git repository.", repo_dir))
  }
}

# Normalize path for downstream code
dir_path <- normalizePath(repo_dir)
message("Using dir_path: ", dir_path)

```


# 3) Choose forecast round (reference date)

```{r choose-round}
# Read submission (reference) dates from hub-config/tasks.json
tasks_path <- file.path(dir_path, "hub-config", "tasks.json")
tasks <- jsonlite::read_json(tasks_path)

# Extract reference dates from the first round / first model task
dates_archive <- unlist(tasks$rounds[[1]]$model_tasks[[1]]$task_ids$reference_date$optional)
dates_archive <- as.Date(dates_archive)
dates_archive <- dates_archive[dates_archive <= Sys.Date()]

# ---- Set reference date of interest ----
# Option 1: most recent submission date
# curr_origin_date <- max(dates_archive, na.rm = TRUE)

# Option 2: explicit date (must be one of the submission dates)
curr_origin_date <- forecast_date

# Option 3: pick by index (e.g., 12th date)
# curr_origin_date <- dates_archive[12]

if (!curr_origin_date %in% dates_archive) {
  stop("Chosen curr_origin_date is not a valid submission date in the Hub.")
}

curr_origin_date
```


# 4) Load model output (submitted forecasts)

```{r read-forecasts}
output_path <- file.path(dir_path, "model-output")

# Retrieve parquet/csv model output files and keep those matching the reference date
file_paths <- list.files(output_path, pattern = "\\.(parquet|csv)$",
                         full.names = TRUE, recursive = TRUE)
file_paths <- file_paths[grepl(curr_origin_date, file_paths)]

if (!length(file_paths)) {
  stop("No model-output files found for curr_origin_date = ", curr_origin_date,
       ". Try a different date.")
}


# helper that picks the right reader
read_model_file <- function(path) {
  if (grepl("\\.parquet(\\.gz)?$", path, ignore.case = TRUE)) {
    # use arrow for parquet (handles both .parquet and .parquet.gz)
    arrow::read_parquet(path)
  } else if (grepl("\\.csv(\\.gz)?$", path, ignore.case = TRUE)) {
    readr::read_csv(path, show_col_types = FALSE)
  } else {
    stop("Unrecognized file type: ", path)
  }
}

# Read & bind; keep quantile forecasts; add model_id from folder name

projection_data_all <- file_paths %>%
  purrr::map_dfr(function(.x) {
    df <- read_model_file(.x)

    # standardize expected columns just in case
    if (!"output_type" %in% names(df))   stop("Missing 'output_type' in: ", .x)
    if (!"output_type_id" %in% names(df)) stop("Missing 'output_type_id' in: ", .x)

    df %>%
      dplyr::filter(.data$output_type == "quantile") %>%
      dplyr::mutate(
        output_type_id = suppressWarnings(as.numeric(.data$output_type_id)),
        model_id = basename(dirname(.x))
      )
  })

prep_proj_data <- projection_data_all %>%
  dplyr::mutate(
    reference_date  = curr_origin_date,
    target_end_date = dplyr::coalesce(target_end_date, reference_date + 7 * as.integer(horizon))
  ) %>%
  dplyr::select(-model,-origin_date)

# Convert to hubverse model_out_tbl format
projection_data_tbl <- hubUtils::as_model_out_tbl(prep_proj_data) %>%
  dplyr::filter(model_id %in% c(
    "FluSight-baseline",
    "MOBS-GLEAM_FLUH",
    "AMPH-SARIMA",
    "AMPH-neuralnetwork",
    "AMPH-epipredict-arx",
    "AMPH-epipredict-climate",
    "FluSight-ensemble"
  ))
  



# Read and join location metadata (for names/abbreviations)
loc_data <- readr::read_csv(file.path(dir_path, "auxiliary-data", "locations.csv"),
                            show_col_types = FALSE)

projection_data_tbl2 <- projection_data_tbl %>%
  dplyr::left_join(
    loc_data %>%
      dplyr::select(location, location_name),
    by = "location"
  ) %>%
  dplyr::mutate(location_name = dplyr::coalesce(location_name, location_id)) %>%
  dplyr::select(-location, -location_id)


dplyr::distinct(projection_data_tbl, model_id)

```



# 5) Load target (observed) data (NO LONGER NEEDED)

```{r read-target}

# # If loading most recent data: 
# target_path <- file.path(dir_path, "target-data", "time-series.csv")
# target_data_all <- readr::read_csv(target_path, show_col_types = FALSE)


# If loading from a specific date: 
date_str <- format(as.Date(curr_origin_date), "%Y-%m-%d")
target_filename <- paste0(date_str, "_flu_target_hospital_admissions_data.csv")

target_path <- file.path(
  dir_path,
  "weekly-summaries",
  date_str,
  target_filename
)

target_data_all <- readr::read_csv(target_path, show_col_types = FALSE)


```

# 6) Pick location, start date, and uncertainty bands

```{r params}
# Location can be "US" or a full state name (must match location_name in target_data)
loc <- "Maryland"
start_date <- as.Date("2024-09-01")
# Middle 50% interval:
uncertainty <- c(0.1, 0.9)
uncertainty

```


# 7) Build a simple equal-weight ensemble

```{r ensemble}
# Filter submitted projections to the location of interest
projection_data <- projection_data_tbl2 %>%
  dplyr::filter(.data$location_name == loc)

# Keep only the chosen forecast round
round_dat <- projection_data %>%
  dplyr::filter(target == "wk inc flu hosp",
                output_type == "quantile",
                horizon >= 0) %>%
  dplyr::collect()

# Generate a simple (equal-weight) ensemble across contributing models
round_ens <- hubEnsembles::simple_ensemble(
  round_dat %>%
    dplyr::filter(!(model_id %in% c("FluSight-baseline",
                                    "FluSight-ensemble",
                                    "AMPH-epipredict-climate")))) %>%
  mutate(model_id = "AMPH-ensemble")

# Combine ensemble with individual models for plotting
plot_df <- dplyr::bind_rows(round_dat, round_ens)

unique(plot_df$model_id)
```



# 8) Prepare data for visualization

```{r viz-prep}
# Forecasts to tidy plot
proj_data <- hubUtils::as_model_out_tbl(plot_df) %>%
  dplyr::rename(target_date = target_end_date) %>%
  dplyr::mutate(output_type_id = suppressWarnings(as.numeric(output_type_id))) %>%
  dplyr::arrange(model_id, horizon, target_date, output_type_id) %>%
  dplyr::distinct(model_id, horizon, target_date, output_type_id, .keep_all = TRUE)


# Observed data for the same location and time window
target_data <- epidata_filter %>%
  dplyr::filter(time_value > start_date) %>%
  dplyr::rename(observation = value,
                date = time_value)

head(proj_data)
head(target_data)

```



# 9) Plot forecasts vs. truth

```{r plot-opt1-hubvis, fig.width=9, fig.height=6}

# This is having issues

hubVis::plot_step_ahead_model_output(
  proj_data,
  target_data,
  use_median_as_point = TRUE,
  show_legend = TRUE,
  intervals = 0.8,        
  ens_name = "AMPH-ensemble",
  ens_color = "black"
)


```


```{r plot-opt2-ggplot, fig.width=9, fig.height=6}
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)

# Identify ensemble id from the object you created earlier
ens_id <- unique(round_ens$model_id)[1]  # e.g., "hub-ensemble"

# Build the 80% ribbon (0.1 / 0.9) for all models
ribbon_80 <- proj_data %>%
  filter(output_type == "quantile", output_type_id %in% c(0.1, 0.9)) %>%
  mutate(output_type_id = as.numeric(output_type_id)) %>%
  select(model_id, horizon, target_date, output_type_id, value) %>%
  pivot_wider(names_from = output_type_id, values_from = value, names_prefix = "q") %>%
  rename(ymin = q0.1, ymax = q0.9)

# Median (0.5) for all models
med_50 <- proj_data %>%
  filter(output_type == "quantile", output_type_id == 0.5) %>%
  select(model_id, horizon, target_date, value) %>%
  mutate(line_width = if_else(model_id == ens_id, 1.1, 0.8))

# Legend order: others first, ensemble last
model_levels <- proj_data %>%
  distinct(model_id) %>%
  pull(model_id) %>%
  setdiff(ens_id) %>%
  c(ens_id)

# Okabe–Ito palette (color-blind friendly)
okabe_ito <- c(
  "#E69F00", "#56B4E9", "#009E73", "#F0E442",
  "#0072B2", "#D55E00", "#CC79A7", "#999999"
)
n_other <- length(model_levels) - 1

other_cols <- if (n_other <= length(okabe_ito)) okabe_ito[seq_len(n_other)] else scales::hue_pal(l = 45, c = 100)(n_other)

# Lines: others = Okabe–Ito, ensemble = black
color_vals <- setNames(c(other_cols, "#000000"), model_levels)
# Ribbons: same hues; ensemble darker gray so black line pops
fill_vals  <- setNames(c(other_cols, "#3A3A3A"), model_levels)

ggplot() +
  geom_ribbon(
    data = ribbon_80,
    aes(x = target_date, ymin = ymin, ymax = ymax, fill = model_id),
    alpha = 0.22, show.legend = TRUE
  ) +
  geom_line(
    data = med_50,
    aes(x = target_date, y = value, color = model_id, linewidth = line_width),
    lineend = "round", alpha = 0.98, show.legend = TRUE
  ) +
  geom_point(
    data = target_data,
    aes(x = date, y = observation),
    size = 1.2, alpha = 0.85, inherit.aes = FALSE,
    color = "grey50"
  ) +
  geom_line(
    data = target_data,
    aes(x = date, y = observation),
    alpha = 0.85, inherit.aes = FALSE,
    color = "grey50"
  ) +
  scale_color_manual(values = color_vals, name = "Model") +
  scale_fill_manual(values  = fill_vals,  name = "Model") +
  scale_linewidth_identity() +
  labs(x = "Target date", y = "Weekly incident hospitalizations") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "right")
```


# 10) Score forecasts (WIS, coverage)

```{r scoring-prep}
scoring_target_data <- epidata %>%
  filter(signal == "confirmed_admissions_flu_ew",
         geo_value %in% used_locations,
         issue_date == curr_origin_date + 28,
         time_value > "2022-09-01") %>%
  select(geo_value, origin_date, value) %>%
  drop_na(value) %>%
  as_epi_df(time_value = origin_date)
```

```{r scoring}
# Join forecasts with observations at (target_date, location)
# and conform to scoringutils "forecast" structure.

scoring_df <- dplyr::left_join(
  proj_data,
  scoring_target_data %>%
    dplyr::rename(observation = value,
                target_date = time_value) %>%
    mutate(location_name = "Maryland") %>%
    select(-geo_value),
  by = c("target_date", "location_name"),
  relationship = "many-to-one"
) %>%
  dplyr::rename(
    model = model_id,
    predicted = value,
    observed = observation,
    quantile_level = output_type_id
  )

# Convert to a scoringutils forecast object
forecast <- scoringutils::as_forecast_quantile(
  scoring_df,
  observed       = "observed",
  predicted      = "predicted",
  quantile_level = "quantile_level",
  # be explicit so extra cols don't confuse the unit of a single forecast
  forecast_unit  = c("model", "location_name", "target_date")
)

# Score (WIS, coverage, etc.)
scores <- scoringutils::score(forecast)

scoringutils::summarise_scores(scores, by = "model")

```



